---
apiVersion: v1
kind: ConfigMap
metadata:
  name: s3-metrics-script
  namespace: platform
data:
  collect-metrics.py: |
    #!/usr/bin/env python3
    import boto3
    import os
    import sys
    import requests
    from botocore.exceptions import ClientError

    def push_metrics(url, metrics):
        """Push metrics to Prometheus Pushgateway"""
        try:
            response = requests.post(url, data=metrics)
            response.raise_for_status()
            return True
        except Exception as e:
            print(f"Error pushing metrics: {e}")
            return False

    def get_bucket_size(s3_client, bucket_name):
        """Get total size and object count for a bucket using pagination"""
        total_size = 0
        total_objects = 0

        try:
            paginator = s3_client.get_paginator('list_objects_v2')
            page_iterator = paginator.paginate(Bucket=bucket_name)

            for page_num, page in enumerate(page_iterator, 1):
                if 'Contents' in page:
                    for obj in page['Contents']:
                        total_size += obj['Size']
                        total_objects += 1
                    print(f"  Page {page_num}: {len(page['Contents'])} objects")

            return total_size, total_objects
        except ClientError as e:
            print(f"Error accessing bucket {bucket_name}: {e}")
            return 0, 0

    def main():
        # Get configuration from environment
        endpoint_url = os.environ.get('AWS_ENDPOINTS', 'https://nbg1.your-objectstorage.com')
        pushgateway_url = os.environ.get('PUSHGATEWAY_URL',
            'http://kube-prometheus-stack-prometheus-pushgateway.platform.svc.cluster.local:9091')

        print(f"Collecting S3 storage metrics from {endpoint_url}")

        # Initialize S3 client
        s3_client = boto3.client(
            's3',
            endpoint_url=endpoint_url,
            aws_access_key_id=os.environ.get('AWS_ACCESS_KEY_ID'),
            aws_secret_access_key=os.environ.get('AWS_SECRET_ACCESS_KEY'),
            region_name=os.environ.get('AWS_DEFAULT_REGION', 'nbg1')
        )

        # List all buckets
        try:
            response = s3_client.list_buckets()
            buckets = response['Buckets']
        except ClientError as e:
            print(f"Error listing buckets: {e}")
            sys.exit(1)

        if not buckets:
            print("No buckets found")
            sys.exit(0)

        print(f"Found {len(buckets)} bucket(s)")

        total_size = 0
        total_objects = 0

        # Process each bucket
        for bucket in buckets:
            bucket_name = bucket['Name']
            print(f"\nProcessing bucket: {bucket_name}")

            size, objects = get_bucket_size(s3_client, bucket_name)
            total_size += size
            total_objects += objects

            size_mb = size / 1024 / 1024
            print(f"Bucket {bucket_name}: {size:,} bytes ({size_mb:.2f} MB), {objects:,} objects")

            # Push per-bucket metrics
            metrics = f"""# TYPE s3_bucket_size_bytes gauge
    # HELP s3_bucket_size_bytes Total size of objects in S3 bucket in bytes
    s3_bucket_size_bytes{{bucket="{bucket_name}"}} {size}
    # TYPE s3_bucket_objects_total gauge
    # HELP s3_bucket_objects_total Total number of objects in S3 bucket
    s3_bucket_objects_total{{bucket="{bucket_name}"}} {objects}
    """
            push_metrics(f"{pushgateway_url}/metrics/job/s3_storage/instance/{bucket_name}", metrics)

        # Summary
        total_gb = total_size / 1024 / 1024 / 1024
        print(f"\n{'='*60}")
        print(f"Total across all buckets:")
        print(f"  Size: {total_size:,} bytes ({total_gb:.2f} GB)")
        print(f"  Objects: {total_objects:,}")
        print(f"{'='*60}")

        # Push total metrics
        total_metrics = f"""# TYPE s3_total_size_bytes gauge
    # HELP s3_total_size_bytes Total size of all objects across all S3 buckets in bytes
    s3_total_size_bytes {total_size}
    # TYPE s3_total_objects gauge
    # HELP s3_total_objects Total number of objects across all S3 buckets
    s3_total_objects {total_objects}
    """

        if push_metrics(f"{pushgateway_url}/metrics/job/s3_storage/instance/total", total_metrics):
            print("\nMetrics pushed successfully to Pushgateway")
        else:
            print("\nFailed to push metrics to Pushgateway")
            sys.exit(1)

    if __name__ == '__main__':
        main()
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: s3-metrics-collector
  namespace: platform
spec:
  # Run every 6 hours
  schedule: "0 */6 * * *"
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 2
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      backoffLimit: 2
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: collector
            image: python:3.11-slim
            command:
            - /bin/sh
            - -c
            - |
              pip install --no-cache-dir boto3 requests > /dev/null 2>&1
              python3 /scripts/collect-metrics.py
            env:
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: s3-backup
                  key: AWS_ACCESS_KEY_ID
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: s3-backup
                  key: AWS_SECRET_ACCESS_KEY
            - name: AWS_ENDPOINTS
              valueFrom:
                secretKeyRef:
                  name: s3-backup
                  key: AWS_ENDPOINTS
            - name: AWS_DEFAULT_REGION
              value: "nbg1"
            - name: PUSHGATEWAY_URL
              value: "http://prometheus-pushgateway.platform.svc.cluster.local:9091"
            volumeMounts:
            - name: scripts
              mountPath: /scripts
            resources:
              requests:
                cpu: 10m
                memory: 32Mi
              limits:
                cpu: 100m
                memory: 128Mi
          volumes:
          - name: scripts
            configMap:
              name: s3-metrics-script
              defaultMode: 0755
# NOTE: s3-backup secret is created during bootstrap (infrastructure/bootstrap/bootstrap.sh)
